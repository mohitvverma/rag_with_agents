{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# async def summarize_content_tool(content: List[Document]) -> str:\n",
    "#     \"\"\"\n",
    "#     Description:\n",
    "#     This function summarizes a list of documents using a state-based summarization pipeline. It extracts summaries from individual documents, merges them iteratively, and generates a final summary.\n",
    "#\n",
    "#     Parameters:\n",
    "#     content (List[Document]): A list of Document objects to be summarized.\n",
    "#\n",
    "#     Returns:\n",
    "#     A str containing the final summarized content.\n",
    "#     \"\"\"\n",
    "#     async def generate_summary(state: SummaryState):\n",
    "#         try:\n",
    "#             response = await initialize_doc_parser_chain().ainvoke(state[\"content\"])\n",
    "#             return {\"summaries\": [response]}\n",
    "#         except Exception as e:\n",
    "#             logger.exception(\"Failed to generate summary\")\n",
    "#             raise e\n",
    "#\n",
    "#     async def collapse_summaries(state: OverallState):\n",
    "#         try:\n",
    "#             doc_lists = split_list_of_docs(\n",
    "#                 state[\"collapsed_summaries\"], length_function, config_settings.MAX_TOKENS\n",
    "#             )\n",
    "#             results = []\n",
    "#             for doc_list in doc_lists:\n",
    "#                 results.append(await acollapse_docs(doc_list, reduce_summary_chain().ainvoke))\n",
    "#             return {\"collapsed_summaries\": results}\n",
    "#         except Exception as e:\n",
    "#             logger.exception(\"Failed to collapse summaries\")\n",
    "#             raise e\n",
    "#\n",
    "#     async def generate_final_summary(state: OverallState):\n",
    "#         try:\n",
    "#             response = await reduce_summary_chain().ainvoke(state[\"collapsed_summaries\"])\n",
    "#             return {\"final_summary\": response}\n",
    "#         except Exception as e:\n",
    "#             logger.exception(\"Failed to generate final summary\")\n",
    "#             raise e\n",
    "#\n",
    "#     try:\n",
    "#         graph = StateGraph(OverallState)\n",
    "#         graph.add_node(\"generate_summary\", generate_summary)\n",
    "#         graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "#         graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "#         graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "#\n",
    "#         graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "#         graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "#         graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "#         graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "#         graph.add_edge(\"generate_final_summary\", END)\n",
    "#\n",
    "#         app = graph.compile()\n",
    "#\n",
    "#         async for step in app.astream(\n",
    "#                 {\"contents\": [doc.page_content for doc in content]},\n",
    "#                 {\"recursion_limit\": 10},\n",
    "#         ):\n",
    "#             if \"final_summary\" in step:\n",
    "#                 return step[\"final_summary\"]\n",
    "#\n",
    "#         raise Exception(\"Failed to generate final summary\")\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         logger.exception(\"Summarization tool failed\")\n",
    "#         raise e\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# async def summarize_content_tool(content: list[Document]) -> str:\n",
    "#     async def generate_summary(state: SummaryState):\n",
    "#         response = await initialize_doc_parser_chain.ainvoke(state[\"content\"])\n",
    "#         return {\"summaries\": [response]}\n",
    "#\n",
    "#     async def collapse_summaries(state: OverallState):\n",
    "#         doc_lists = split_list_of_docs(\n",
    "#             state[\"collapsed_summaries\"], length_function, config_settings.MAX_TOKENS\n",
    "#         )\n",
    "#         results = []\n",
    "#         for doc_list in doc_lists:\n",
    "#             results.append(await acollapse_docs(doc_list, reduce_summary_chain.ainvoke))\n",
    "#\n",
    "#         return {\"collapsed_summaries\": results}\n",
    "#\n",
    "#     async def generate_final_summary(state: OverallState):\n",
    "#         response = await reduce_summary_chain.ainvoke(state[\"collapsed_summaries\"])\n",
    "#         return {\"final_summary\": response}\n",
    "#\n",
    "#     graph = StateGraph(OverallState)\n",
    "#     graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
    "#     graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "#     graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "#     graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "#\n",
    "#     # Edges:\n",
    "#     graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "#     graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "#     graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "#     graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "#     graph.add_edge(\"generate_final_summary\", END)\n",
    "#\n",
    "#     app = graph.compile()\n",
    "#\n",
    "#     async for step in app.astream(\n",
    "#             {\"contents\": [doc.page_content for doc in content]},\n",
    "#             {\"recursion_limit\": 10},\n",
    "#     ):\n",
    "#         print(list(step.keys()))\n",
    "#"
   ],
   "id": "ef6c066f987466ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import asyncio\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from loguru import logger\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from domains.utils import get_chat_model\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from domains.agents.models import QueryRequest, OverallState, SummaryState\n",
    "from domains.agents.tools import qna_tool, information_extraction_tool, summarize_content_tool\n",
    "from domains.settings import config_settings\n",
    "\n",
    "\n",
    "async def orchestrator_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Orchestrator agent that uses qna_tool, information_extraction_tool, and summarize_content_tool.\n",
    "\n",
    "    Args:\n",
    "        query: The query string.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the final summary.\n",
    "    \"\"\"\n",
    "    async def run_qna_tool(state: OverallState):\n",
    "        try:\n",
    "            request = QueryRequest(query=state['query'])\n",
    "            documents = await qna_tool(request)\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to run qna_tool\")\n",
    "            raise e\n",
    "\n",
    "    async def run_information_extraction_tool(state: OverallState):\n",
    "        try:\n",
    "            documents = await information_extraction_tool(query=state['query'])\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to run information_extraction_tool\")\n",
    "            raise e\n",
    "\n",
    "    async def run_summarize_content_tool(state: OverallState):\n",
    "        try:\n",
    "            summary = await summarize_content_tool(state[\"documents\"])\n",
    "            return {\"final_summary\": summary}\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Failed to run summarize_content_tool\")\n",
    "            raise e\n",
    "\n",
    "    try:\n",
    "        graph = StateGraph(OverallState)\n",
    "        graph.add_node(\"run_qna_tool\", run_qna_tool)\n",
    "        graph.add_node(\"run_information_extraction_tool\", run_information_extraction_tool)\n",
    "        graph.add_node(\"run_summarize_content_tool\", run_summarize_content_tool)\n",
    "\n",
    "        graph.add_edge(START, \"run_qna_tool\")\n",
    "        graph.add_conditional_edges(\"run_qna_tool\", lambda state: \"run_information_extraction_tool\" if not state[\"documents\"] else \"run_summarize_content_tool\")\n",
    "        graph.add_edge(\"run_information_extraction_tool\", \"run_summarize_content_tool\")\n",
    "        graph.add_edge(\"run_summarize_content_tool\", END)\n",
    "\n",
    "        app = graph.compile()\n",
    "\n",
    "        async for step in app.astream({}, {\"recursion_limit\": 10}):\n",
    "            if \"final_summary\" in step:\n",
    "                return step[\"final_summary\"]\n",
    "\n",
    "        raise Exception(\"Failed to generate final summary\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Orchestrator agent failed\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "async def new(query: str, id):\n",
    "    agent_executor=create_react_agent(\n",
    "        model=get_chat_model(model_key=\"OPENAI_CHAT\"),\n",
    "        tools=[orchestrator_agent],\n",
    "        checkpointer=memory,\n",
    "    )\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": id}}\n",
    "    async for step in agent_executor.astream(\n",
    "            {\"messages\": [HumanMessage(content=query)]},\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "    ):\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "\n"
   ],
   "id": "b53dd9f525e3c942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# async def run_agents_2(query: str, id: str):\n",
    "#     # Define custom state schema\n",
    "#     class AgentState(TypedDict):\n",
    "#         messages: Annotated[list[BaseMessage], add_messages]  # Chat messages\n",
    "#         query: str  # Original query\n",
    "#         result: Optional[str]  # Orchestrator result\n",
    "#         is_last_step: IsLastStep\n",
    "#         remaining_steps: RemainingSteps\n",
    "#\n",
    "#     # Create memory for state persistence\n",
    "#     memory = MemorySaver()\n",
    "#\n",
    "#     # Create tool node for orchestrator\n",
    "#     async def run_orchestrator(state: AgentState) -> AgentState:\n",
    "#         \"\"\"Execute orchestrator and update state with result\"\"\"\n",
    "#         try:\n",
    "#             result = await orchestrator_agent(state[\"query\"])\n",
    "#             # Handle case where result is a dictionary\n",
    "#             if isinstance(result, dict) and \"contents\" in result:\n",
    "#                 content = result[\"contents\"][0] if isinstance(result[\"contents\"], list) else str(result[\"contents\"])\n",
    "#             else:\n",
    "#                 content = str(result)\n",
    "#\n",
    "#             return {\n",
    "#                 \"messages\": [AIMessage(content=content)],\n",
    "#                 \"result\": content\n",
    "#             }\n",
    "#\n",
    "#         except Exception as e:\n",
    "#             logger.exception(\"Orchestrator execution failed\")\n",
    "#             error_msg = f\"Orchestrator failed: {str(e)}\"\n",
    "#             return {\n",
    "#                 \"messages\": [AIMessage(content=error_msg)],\n",
    "#                 \"result\": error_msg\n",
    "#             }\n",
    "#     # Create workflow graph\n",
    "#     workflow = StateGraph(AgentState)\n",
    "#\n",
    "#     # Add nodes\n",
    "#     workflow.add_node(\"orchestrator\", run_orchestrator)\n",
    "#\n",
    "#     # Set entry point and flow\n",
    "#     workflow.set_entry_point(\"orchestrator\")\n",
    "#     workflow.add_edge(\"orchestrator\", END)\n",
    "#\n",
    "#     # Compile graph\n",
    "#     agent_executor = workflow.compile(checkpointer=memory)\n",
    "#\n",
    "#     # Execute graph\n",
    "#     config = {\"configurable\": {\"thread_id\": id}}\n",
    "#     final_result = None\n",
    "#\n",
    "#     # Stream results\n",
    "#     async for step in agent_executor.astream(\n",
    "#             {\n",
    "#                 \"messages\": [HumanMessage(content=query)],\n",
    "#                 \"query\": query,\n",
    "#                 \"result\": None\n",
    "#             },\n",
    "#             config\n",
    "#     ):\n",
    "#         if \"result\" in step:\n",
    "#             final_result = step[\"result\"]\n",
    "#\n",
    "#     logger.info(f\"Agent result: {final_result}\")\n",
    "#     return final_result"
   ],
   "id": "a67df8c49053cdc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# class StreamingLLMCallbackHandler(AsyncCallbackHandler):\n",
    "#     \"\"\"Callback handler for streaming LLM responses.\"\"\"\n",
    "#\n",
    "#     def __init__(self, websocket_internal: WebSocket):\n",
    "#         self.websocket = websocket_internal\n",
    "#\n",
    "#     async def on_chat_model_start(\n",
    "#             self,\n",
    "#             serialized: typing.Dict[str, typing.Any],\n",
    "#             messages: typing.List[typing.List[BaseMessage]],\n",
    "#             *,\n",
    "#             run_id: uuid.UUID,\n",
    "#             parent_run_id: uuid.UUID | None = None,\n",
    "#             tags: typing.List[str] | None = None,\n",
    "#             metadata: typing.Dict[str, typing.Any] | None = None,\n",
    "#             **kwargs: typing.Any,\n",
    "#     ) -> typing.Any:\n",
    "#         logger.info(f\"LLM chain chat model start with serialized: {serialized}\\nmessages: {messages}\\nkwargs: {kwargs}\")\n",
    "#\n",
    "#     async def on_llm_start(self,\n",
    "#                            serialized: typing.Dict[str, typing.Any],\n",
    "#                            prompts: typing.List[str],\n",
    "#                            *,\n",
    "#                            run_id: uuid.UUID,\n",
    "#                            parent_run_id: typing.Optional[uuid.UUID] = None,\n",
    "#                            tags: typing.Optional[typing.List[str]] = None,\n",
    "#                            metadata: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "#                            **kwargs: typing.Any) -> None:\n",
    "#         logger.info(f'LLM chain started with prompts: {prompts} and kwargs: {kwargs}')\n",
    "#\n",
    "#     async def on_llm_new_token(self, token: str, **kwargs: typing.Any) -> None:\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "#         resp = {\n",
    "#             \"message\": token,\n",
    "#             \"type\": \"stream\",\n",
    "#             \"content_type\": None\n",
    "#         }\n",
    "#         formatted_resp = f'{resp}\\n{timestamp}'\n",
    "#         await self.websocket.send_json(formatted_resp)\n",
    "#\n",
    "#     async def on_llm_end(self, response: LLMResult, *, run_id: uuid.UUID,\n",
    "#                          parent_run_id: typing.Optional[uuid.UUID] = None,\n",
    "#                          tags: typing.Optional[typing.List[str]] = None, **kwargs: typing.Any) -> None:\n",
    "#         logger.info(f'LLM chain ended with response: {response}')\n",
    "\n",
    "\n",
    "# class StreamingLLMCallbackHandler(langchain.callbacks.base.AsyncCallbackHandler):\n",
    "#     \"\"\"Callback handler for streaming LLM responses.\"\"\"\n",
    "#\n",
    "#     def __init__(self, websocket_internal):\n",
    "#         self.websocket = websocket_internal\n",
    "#\n",
    "#     async def on_chat_model_start(\n",
    "#             self,\n",
    "#             serialized: typing.Dict[str, typing.Any],\n",
    "#             messages: typing.List[typing.List[BaseMessage]],\n",
    "#             *,\n",
    "#             run_id: uuid.UUID,\n",
    "#             parent_run_id: uuid.UUID | None = None,\n",
    "#             tags: typing.List[str] | None = None,\n",
    "#             metadata: typing.Dict[str, typing.Any] | None = None,\n",
    "#             **kwargs: typing.Any,\n",
    "#     ) -> typing.Any:\n",
    "#         logger.info(f\"LLM chain chat model start with serialized: {serialized}\\nmessages: {messages}\\nkwargs: {kwargs}\")\n",
    "#\n",
    "#     async def on_llm_start(self,\n",
    "#                            serialized: typing.Dict[str, typing.Any],\n",
    "#                            prompts: typing.List[str],\n",
    "#                            *,\n",
    "#                            run_id: uuid.UUID,\n",
    "#                            parent_run_id: typing.Optional[uuid.UUID] = None,\n",
    "#                            tags: typing.Optional[typing.List[str]] = None,\n",
    "#                            metadata: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "#                            **kwargs: typing.Any) -> None:\n",
    "#         logger.info(f'LLM chain started with prompts: {prompts} and kwargs: {kwargs}')\n",
    "#\n",
    "#     async def on_llm_new_token(self, token: str, **kwargs: typing.Any) -> None:\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "#         resp = {\n",
    "#             \"message\": token,\n",
    "#             \"type\": \"stream\",\n",
    "#             \"content_type\": None\n",
    "#         }\n",
    "#         formatted_resp = f'{resp}\\n{timestamp}'\n",
    "#         await self.websocket.send_json(formatted_resp)\n",
    "#\n",
    "#     async def on_llm_end(self, response: langchain.schema.output.LLMResult, *, run_id: uuid.UUID,\n",
    "#                          parent_run_id: typing.Optional[uuid.UUID] = None,\n",
    "#                          tags: typing.Optional[typing.List[str]] = None, **kwargs: typing.Any) -> None:\n",
    "#         logger.info(f'LLM chain ended with response: {response}')\n",
    "\n",
    "# class StreamingLLMCallbackHandler(langchain.callbacks.base.AsyncCallbackHandler):\n",
    "#     \"\"\"Callback handler for streaming LLM responses.\"\"\"\n",
    "#\n",
    "#     def __init__(self, websocket_internal):\n",
    "#         self.websocket = websocket_internal\n",
    "#\n",
    "#     async def on_chat_model_start(\n",
    "#             self,\n",
    "#             serialized: typing.Dict[str, typing.Any],\n",
    "#             messages: typing.List[typing.List[BaseMessage]],\n",
    "#             *,\n",
    "#             run_id: uuid.UUID,\n",
    "#             parent_run_id: uuid.UUID | None = None,\n",
    "#             tags: typing.List[str] | None = None,\n",
    "#             metadata: typing.Dict[str, typing.Any] | None = None,\n",
    "#             **kwargs: typing.Any,\n",
    "#     ) -> typing.Any:\n",
    "#         logger.info(f\"LLM chain chat model start with serialized: {serialized}\\nmessages: {messages}\\nkwargs: {kwargs}\")\n",
    "#\n",
    "#     async def on_llm_start(self,\n",
    "#                            serialized: typing.Dict[str, typing.Any],\n",
    "#                            prompts: typing.List[str],\n",
    "#                            *,\n",
    "#                            run_id: uuid.UUID,\n",
    "#                            parent_run_id: typing.Optional[uuid.UUID] = None,\n",
    "#                            tags: typing.Optional[typing.List[str]] = None,\n",
    "#                            metadata: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "#                            **kwargs: typing.Any) -> None:\n",
    "#         logger.info(f'LLM chain started with prompts: {prompts} and kwargs: {kwargs}')\n",
    "#\n",
    "#     async def on_llm_new_token(self, token: str, **kwargs: typing.Any) -> None:\n",
    "#         resp = ChatResponse(message=token, type=\"stream\")\n",
    "#         await self.websocket.send_json(resp.dict())\n",
    "#\n",
    "#     async def on_llm_end(self, response: langchain.schema.output.LLMResult, *, run_id: uuid.UUID,\n",
    "#                          parent_run_id: typing.Optional[uuid.UUID] = None,\n",
    "#                          tags: typing.Optional[typing.List[str]] = None, **kwargs: typing.Any) -> None:\n",
    "#         logger.info(f'LLM chain ended with response: {response}')"
   ],
   "id": "3fda1e3354ca0733"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import pprint\n",
    "#\n",
    "# import fastapi\n",
    "# from pydantic import BaseModel\n",
    "#\n",
    "# from domains.retreival.rag_util import send_message_over_websocket\n",
    "# from domains import retreival\n",
    "# from domains.retreival.utils import transform_user_query_for_retreival\n",
    "# from fastapi import WebSocket\n",
    "# from langchain_core.documents import Document\n",
    "#\n",
    "# from domains.retreival.pinecone_doc_retreival.utils import get_related_docs_without_context\n",
    "# from typing import Optional, List, Dict, Callable\n",
    "# from domains.retreival.initialize_memory import initialise_memory_from_chat_context\n",
    "# from domains.settings import config_settings\n",
    "#\n",
    "# from domains.retreival.utils import get_chat_model_with_streaming\n",
    "# from langchain.chains import question_answering as q_a\n",
    "#\n",
    "# from loguru import logger\n",
    "# from domains.retreival.models import RagUseCase, RAGGenerationResponse\n",
    "# from domains.retreival.models import Message\n",
    "# from domains.retreival.prompts import PROMPT_PREFIX_QNA, PROMPT_SUFFIX, initialise_doc_search_prompt_template\n",
    "# from langchain.prompts import PromptTemplate\n",
    "#\n",
    "#\n",
    "#\n",
    "# def run_rag(\n",
    "#     question: str,\n",
    "#     language: str,\n",
    "#     chat_context: Optional[List[Message]],\n",
    "#     websocket: fastapi.WebSocket,\n",
    "#     namespace: str,\n",
    "# ):\n",
    "#     DEFAULT_MIN_SCORE = 0.8\n",
    "#     # initialise minimum score\n",
    "#     minimum_score = (\n",
    "#         config_settings.MINIMUM_SCORE\n",
    "#         or DEFAULT_MIN_SCORE\n",
    "#     )\n",
    "#\n",
    "#     # initialise prompt\n",
    "#     prompt_qna_ask_question = initialise_doc_search_prompt_template(\n",
    "#         PROMPT_PREFIX_QNA, PROMPT_SUFFIX\n",
    "#     )\n",
    "#\n",
    "#     # initialise memory\n",
    "#     memory = initialise_memory_from_chat_context(\n",
    "#         chat_context\n",
    "#     )\n",
    "#\n",
    "#     if not namespace or namespace == \"\":\n",
    "#         namespace = config_settings.PINECONE_DEFAULT_DEV_NAMESPACE\n",
    "#\n",
    "#     return rag_with_streaming(\n",
    "#         websocket=websocket,\n",
    "#         language=language,\n",
    "#         question=question,\n",
    "#         minimum_score=minimum_score,\n",
    "#         prompt_template_ask_question=prompt_qna_ask_question,\n",
    "#         memory=memory,\n",
    "#         namespace=namespace,\n",
    "#     )\n",
    "#\n",
    "#\n",
    "# async def rag_with_streaming(\n",
    "#     websocket: fastapi.WebSocket,\n",
    "#     question: str,\n",
    "#     language: str,\n",
    "#     minimum_score: float,\n",
    "#     prompt_template_ask_question: PromptTemplate,\n",
    "#     memory,\n",
    "#     namespace: str,\n",
    "#     use_case: RagUseCase = RagUseCase.DEFAULT,\n",
    "#     citations_count=0,\n",
    "# ):\n",
    "#     try:\n",
    "#         if not citations_count:\n",
    "#             citations_count = config_settings.PINECONE_TOTAL_DOCS_TO_RETRIEVE\n",
    "#\n",
    "#         # citations_toggle = config_settings.CITATIONS_TOGGLE\n",
    "#         index_name = config_settings.PINECONE_INDEX_NAME\n",
    "#\n",
    "#         await send_message_over_websocket(\n",
    "#             websocket, \"\", retreival.MESSAGE_TYPE_START\n",
    "#         )\n",
    "#\n",
    "#         await send_message_over_websocket(\n",
    "#             websocket,\n",
    "#             \"\",\n",
    "#             retreival.MESSAGE_TYPE_START,\n",
    "#             content_type=retreival.CONTENT_TYPE_OPTIMISED_QUESTION,\n",
    "#         )\n",
    "#\n",
    "#         retreival_query = await transform_user_query_for_retreival(\n",
    "#             question, \"OPTIMIZED_QUESTION_MODEL\"\n",
    "#         )\n",
    "#\n",
    "#         logger.debug(f\"Retreival query: {retreival_query}\")\n",
    "#\n",
    "#         related_docs_with_score = []\n",
    "#         if retreival_query or retreival_query != \"None\":\n",
    "#             related_docs_with_score = await get_related_docs_without_context(\n",
    "#                 index_name,\n",
    "#                 namespace,\n",
    "#                 retreival_query,\n",
    "#             )\n",
    "#\n",
    "#             # logger.debug(\n",
    "#             #     \"\\n\".join(\n",
    "#             #         [\n",
    "#             #             f'File Name: {doc.metadata.get(\"file_name\", \"\")}, Score: {score}, Page Number: {doc.metadata.get(\"page\", 0)}'\n",
    "#             #             for doc, score in related_docs_with_score\n",
    "#             #         ]\n",
    "#             #     )\n",
    "#             # )\n",
    "#         logger.info(f\"Related docs without context: {related_docs_with_score}\")\n",
    "#         await send_message_over_websocket(\n",
    "#             websocket,\n",
    "#             \"\",\n",
    "#             retreival.MESSAGE_TYPE_END,\n",
    "#             content_type=retreival.CONTENT_TYPE_OPTIMISED_QUESTION,\n",
    "#         )\n",
    "#\n",
    "#         route = RagUseCase.DEFAULT\n",
    "#\n",
    "#         rag_generation: RAGGenerationResponse = await generator_routing(\n",
    "#             memory,\n",
    "#             language,\n",
    "#             retreival_query,\n",
    "#             prompt_template_ask_question,\n",
    "#             websocket,\n",
    "#             route,\n",
    "#             citations_count,\n",
    "#             minimum_score,\n",
    "#             related_docs_with_score,\n",
    "#         )\n",
    "#\n",
    "#         await send_message_over_websocket(\n",
    "#             websocket, \"\", retreival.MESSAGE_TYPE_END, content_type=retreival.CONTENT_TYPE_ANSWER\n",
    "#         )\n",
    "#\n",
    "#\n",
    "#         await send_message_over_websocket(\n",
    "#             websocket, \"\", retreival.MESSAGE_TYPE_END\n",
    "#         )\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         await send_message_over_websocket(\n",
    "#             websocket, f\"Error: {e}\", retreival.MESSAGE_TYPE_ERROR\n",
    "#         )\n",
    "#         logger.error(f\"Error: {e}\")\n",
    "#         return\n",
    "#\n",
    "#\n",
    "# async def generator_routing(\n",
    "#     memory,\n",
    "#     language: str,\n",
    "#     optimised_question: str,\n",
    "#     prompt_template_ask_question: PromptTemplate,\n",
    "#     websocket: WebSocket,\n",
    "#     route: str,\n",
    "#     citations_count: int,\n",
    "#     minimum_score: float,\n",
    "#     related_docs_with_score: list[tuple[Document, float]] = [],\n",
    "# ) -> RAGGenerationResponse:\n",
    "#\n",
    "#     if route == RagUseCase.DEFAULT:\n",
    "#         response = await run_doc_retrieval_flow(\n",
    "#             memory,\n",
    "#             optimised_question,\n",
    "#             prompt_template_ask_question,\n",
    "#             related_docs_with_score[:citations_count],\n",
    "#             websocket,\n",
    "#             minimum_score,\n",
    "#             language,\n",
    "#         )\n",
    "#\n",
    "#     return response\n",
    "#\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "#\n",
    "# async def run_doc_retrieval_flow(\n",
    "#     memory,\n",
    "#     optimised_question: str,\n",
    "#     prompt_template_ask_question: PromptTemplate,\n",
    "#     related_docs_with_score: list[tuple[Document, float]],\n",
    "#     websocket: WebSocket,\n",
    "#     minimum_score: float,\n",
    "#     language: str,\n",
    "# ) -> RAGGenerationResponse:\n",
    "#\n",
    "#     # document_count = len(\n",
    "#     #     [doc for doc in related_docs_with_score if doc[1] > minimum_score]\n",
    "#     # )\n",
    "#\n",
    "#     document_count = len(related_docs_with_score)\n",
    "#\n",
    "#     llm =get_chat_model_with_streaming(\n",
    "#         websocket, model_key=config_settings.LLMS.get(\"OPENAI_CHAT\")\n",
    "#     )\n",
    "#     if llm:\n",
    "#         llm_chain = prompt_template_ask_question | llm | StrOutputParser()\n",
    "#\n",
    "#         response = await llm_chain.ainvoke(\n",
    "#             {\n",
    "#                 \"question\": optimised_question,\n",
    "#                 \"chat_history\": memory.buffer_as_str,\n",
    "#                 \"doc_count\": str(document_count),\n",
    "#                 \"context\": related_docs_with_score,\n",
    "#                 \"language\": language\n",
    "#             },\n",
    "#         )\n",
    "#         logger.info(f\"Response: {response}\")\n",
    "#\n",
    "#         return RAGGenerationResponse(\n",
    "#             answer=response\n",
    "#         )"
   ],
   "id": "f825a9fb54f7ec25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
